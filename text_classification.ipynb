{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64acc7fb-59de-4ebc-b82f-afb53cc852d2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3379e5ef-2708-48c0-a8ed-95eda57a8aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 23:12:56.855946: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733490776.996709  150544 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733490777.031121  150544 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-06 23:12:57.339425: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "import tqdm.notebook as tqdm\n",
    "import sklearn.model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import csv\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertModel, BertConfig\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1883d74d-c3a4-483e-a014-aef8581fc020",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3218b2db-cb61-42be-8368-649764b08b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "EXAMPLES_COUNT = 50000\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "EPOCH_COUNT=20\n",
    "\n",
    "CLASS_COUNT=4\n",
    "\n",
    "ALL_BATCHES = EXAMPLES_COUNT // BATCH_SIZE\n",
    "TRAIN_BATCHES = int(ALL_BATCHES * 0.8)\n",
    "TEST_BATCHES = (ALL_BATCHES - TRAIN_BATCHES) // 2\n",
    "TEST_BATCHES = TEST_BATCHES\n",
    "\n",
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3588f5af-a33c-4638-90d8-5cb912535790",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c468a19b-1395-4afc-b0c5-a799160ea671",
   "metadata": {},
   "source": [
    "Load and prepare your dataset. Dataset should have at least 10k samples in it. Each dataset cannot be used by more than two students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e177b592-0e36-46f0-a940-e3c2daa25de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_CATEGORY = 'category'\n",
    "COLUMN_DESCRIPTION = 'description'\n",
    "\n",
    "LOCAL_CSV_FILE_NAME = 'ecommerce-dataset.csv'\n",
    "LOCAL_CSV_FILE_PATH = f'./{LOCAL_CSV_FILE_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1258a10d-9d7a-41d8-8f2f-d0ee45f65626",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['Household', 'Books', 'Clothing & Accessories', 'Electronics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "27724773-5172-4a0f-87ab-6878341a7ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_csv():\n",
    "    DATASET_URL = 'https://zenodo.org/record/3355823/files/ecommerceDataset.csv?download=1'\n",
    "    \n",
    "    df = pd.read_csv(DATASET_URL, names=[COLUMN_CATEGORY, COLUMN_DESCRIPTION], header=None, index_col=False)\n",
    "\n",
    "    df.dropna()\n",
    "    df.drop_duplicates()\n",
    "\n",
    "    codes = df[COLUMN_CATEGORY].replace(['Household', 'Books', 'Clothing & Accessories', 'Electronics'], [0, 1, 2, 3])\n",
    "    df[COLUMN_CATEGORY] = codes.astype(int)\n",
    "    df[COLUMN_DESCRIPTION] = df[COLUMN_DESCRIPTION].astype(str)\n",
    "\n",
    "    df = df.drop(index=39330)\n",
    "\n",
    "    df.to_csv(LOCAL_CSV_FILE_NAME, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9efdc1d2-bf23-4a32-b169-ecf8585c2657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63793/3767133491.py:9: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  codes = df[COLUMN_CATEGORY].replace(['Household', 'Books', 'Clothing & Accessories', 'Electronics'], [0, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "load_data_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bac91d0e-10e6-43c0-861b-85fa28217934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_data():\n",
    "    df = pd.read_csv(LOCAL_CSV_FILE_PATH, index_col=False)\n",
    "\n",
    "    y = df[COLUMN_CATEGORY].to_numpy()[:EXAMPLES_COUNT]\n",
    "    X = df[COLUMN_DESCRIPTION].to_numpy()[:EXAMPLES_COUNT]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acda18df-85cf-4340-acb4-971502b16928",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714bddcc-c56d-4e61-9147-054f5d7f8e7e",
   "metadata": {},
   "source": [
    "# Backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a2f49-08a6-479a-8db7-a52d4db40d56",
   "metadata": {},
   "source": [
    "Load pretrained model from Hugging Face (or some other model repository if it's more convenient). Model should be trained on Feature Extraction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd55f5f4-3a8e-41ea-b5ed-091977404843",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_SRC = \"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d111bc81-dfb4-47fb-b3d8-42388b84a8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733490810.296723  150544 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1414 MB memory:  -> device: 0, name: NVIDIA GeForce GT 1030, pci bus id: 0000:09:00.0, compute capability: 6.1\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "backbone = transformers.TFAutoModel.from_pretrained(BERT_SRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "640a37ec-5972-43c1-bfca-e31d19b758fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b81337a-c06e-4b33-9dff-61ff0c6858c9",
   "metadata": {},
   "source": [
    "Load tokenizer to be used with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c7e98fb-a356-463d-a856-9b011efdcca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(BERT_SRC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c045f3c1-faf9-41fc-af88-a4f49c97ac13",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1025d0d6-3b43-440a-8791-27b3e208e415",
   "metadata": {},
   "source": [
    "Since we will not be training the backbone, extract features from your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fb0bf5-6e28-49fa-a0c3-e5b3fd51e7f7",
   "metadata": {},
   "source": [
    "Tokenize all your sequences. Truncate/pad the squences for convenience. If the sequences are too large to be stored in memory, lazily save them on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8a9c921-8843-42c1-95fd-75d01878621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(texts):\n",
    "    tokens = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a46eb89f-21ac-4a08-a32c-e604619f3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECCOM_DATA_FEATURES_FILE = \"ECCOM_DATA_FEATURES-v3.h5\"\n",
    "ECCOM_DATA_FEATURES_DATASET = \"ECCOM_DATA_FEATURES_DATASET\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adef1d6b-4095-4447-908c-5d86c5dd8c78",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 1561/1562 [2:33:24<00:05,  5.78s/it]2024-12-07 01:47:43.350952: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:497] Allocator (GPU_0_bfc) ran out of memory trying to allocate 144.00MiB (rounded to 150994944)requested by op Softmax\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2024-12-07 01:47:43.351005: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1053] BFCAllocator dump for GPU_0_bfc\n",
      "2024-12-07 01:47:43.351017: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (256): \tTotal Chunks: 12, Chunks in use: 12. 3.0KiB allocated for chunks. 3.0KiB in use in bin. 52B client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351022: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351028: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351034: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (2048): \tTotal Chunks: 111, Chunks in use: 111. 333.0KiB allocated for chunks. 333.0KiB in use in bin. 333.0KiB client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351039: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (4096): \tTotal Chunks: 1, Chunks in use: 1. 6.0KiB allocated for chunks. 6.0KiB in use in bin. 6.0KiB client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351044: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (8192): \tTotal Chunks: 13, Chunks in use: 12. 157.2KiB allocated for chunks. 147.0KiB in use in bin. 144.0KiB client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351048: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351054: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (32768): \tTotal Chunks: 2, Chunks in use: 2. 96.0KiB allocated for chunks. 96.0KiB in use in bin. 96.0KiB client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351059: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (65536): \tTotal Chunks: 4, Chunks in use: 3. 368.0KiB allocated for chunks. 272.0KiB in use in bin. 240.0KiB client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351063: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (131072): \tTotal Chunks: 1, Chunks in use: 0. 137.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351068: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351072: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351077: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (1048576): \tTotal Chunks: 2, Chunks in use: 1. 3.64MiB allocated for chunks. 1.99MiB in use in bin. 1.50MiB client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351083: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (2097152): \tTotal Chunks: 50, Chunks in use: 49. 112.79MiB allocated for chunks. 110.54MiB in use in bin. 110.25MiB client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351087: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (4194304): \tTotal Chunks: 1, Chunks in use: 0. 6.75MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351093: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (8388608): \tTotal Chunks: 26, Chunks in use: 24. 241.75MiB allocated for chunks. 223.75MiB in use in bin. 216.00MiB client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351098: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (16777216): \tTotal Chunks: 1, Chunks in use: 1. 31.58MiB allocated for chunks. 31.58MiB in use in bin. 24.00MiB client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351103: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (33554432): \tTotal Chunks: 6, Chunks in use: 5. 224.42MiB allocated for chunks. 180.00MiB in use in bin. 180.00MiB client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351111: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (67108864): \tTotal Chunks: 1, Chunks in use: 1. 89.42MiB allocated for chunks. 89.42MiB in use in bin. 89.42MiB client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351116: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (134217728): \tTotal Chunks: 5, Chunks in use: 3. 703.58MiB allocated for chunks. 432.00MiB in use in bin. 432.00MiB client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351120: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-07 01:47:43.351125: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1076] Bin for 144.00MiB was 128.00MiB, Chunk State: \n",
      "2024-12-07 01:47:43.351134: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1082]   Size: 134.58MiB | Requested Size: 48.0KiB | in_use: 0 | bin_num: 19, prev:   Size: 144.00MiB | Requested Size: 144.00MiB | in_use: 1 | bin_num: -1\n",
      "2024-12-07 01:47:43.351140: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1082]   Size: 137.00MiB | Requested Size: 24.00MiB | in_use: 0 | bin_num: 19, prev:   Size: 144.00MiB | Requested Size: 144.00MiB | in_use: 1 | bin_num: -1\n",
      "2024-12-07 01:47:43.351144: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1089] Next region of size 2097152\n",
      "2024-12-07 01:47:43.351150: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501a00000 of size 256 next 1\n",
      "2024-12-07 01:47:43.351154: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501a00100 of size 1280 next 2\n",
      "2024-12-07 01:47:43.351158: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501a00600 of size 256 next 3\n",
      "2024-12-07 01:47:43.351161: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501a00700 of size 256 next 4\n",
      "2024-12-07 01:47:43.351165: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501a00800 of size 256 next 6\n",
      "2024-12-07 01:47:43.351168: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501a00900 of size 3072 next 11\n",
      "2024-12-07 01:47:43.351172: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501a01500 of size 256 next 15\n",
      "2024-12-07 01:47:43.351176: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501a01600 of size 3072 next 16\n",
      "2024-12-07 01:47:43.351180: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501a02200 of size 3072 next 19\n",
      "2024-12-07 01:47:43.351184: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501a02e00 of size 3072 next 12\n",
      "2024-12-07 01:47:43.351188: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501a03a00 of size 2082304 next 18446744073709551615\n",
      "2024-12-07 01:47:43.351192: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1089] Next region of size 134217728\n",
      "2024-12-07 01:47:43.351196: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c00000 of size 3072 next 18\n",
      "2024-12-07 01:47:43.351200: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c00c00 of size 3072 next 25\n",
      "2024-12-07 01:47:43.351203: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c01800 of size 3072 next 22\n",
      "2024-12-07 01:47:43.351231: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c02400 of size 3072 next 17\n",
      "2024-12-07 01:47:43.351235: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c03000 of size 3072 next 29\n",
      "2024-12-07 01:47:43.351240: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c03c00 of size 3072 next 32\n",
      "2024-12-07 01:47:43.351249: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c04800 of size 3072 next 26\n",
      "2024-12-07 01:47:43.351257: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c05400 of size 3072 next 215\n",
      "2024-12-07 01:47:43.351262: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c06000 of size 3072 next 36\n",
      "2024-12-07 01:47:43.351266: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c06c00 of size 3072 next 38\n",
      "2024-12-07 01:47:43.351269: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c07800 of size 3072 next 40\n",
      "2024-12-07 01:47:43.351273: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c08400 of size 3072 next 42\n",
      "2024-12-07 01:47:43.351277: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c09000 of size 15360 next 35\n",
      "2024-12-07 01:47:43.351281: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c0cc00 of size 3072 next 34\n",
      "2024-12-07 01:47:43.351285: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c0d800 of size 3072 next 44\n",
      "2024-12-07 01:47:43.351289: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c0e400 of size 3072 next 45\n",
      "2024-12-07 01:47:43.351293: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c0f000 of size 3072 next 48\n",
      "2024-12-07 01:47:43.351297: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c0fc00 of size 3072 next 50\n",
      "2024-12-07 01:47:43.351301: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c10800 of size 3072 next 53\n",
      "2024-12-07 01:47:43.351304: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c11400 of size 3072 next 55\n",
      "2024-12-07 01:47:43.351308: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c12000 of size 3072 next 57\n",
      "2024-12-07 01:47:43.351312: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c12c00 of size 3072 next 51\n",
      "2024-12-07 01:47:43.351316: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c13800 of size 12288 next 60\n",
      "2024-12-07 01:47:43.351320: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c16800 of size 3072 next 52\n",
      "2024-12-07 01:47:43.351324: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c17400 of size 3072 next 61\n",
      "2024-12-07 01:47:43.351328: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c18000 of size 3072 next 62\n",
      "2024-12-07 01:47:43.351331: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c18c00 of size 3072 next 33\n",
      "2024-12-07 01:47:43.351335: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c19800 of size 3072 next 70\n",
      "2024-12-07 01:47:43.351339: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c1a400 of size 3072 next 72\n",
      "2024-12-07 01:47:43.351343: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c1b000 of size 3072 next 74\n",
      "2024-12-07 01:47:43.351347: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c1bc00 of size 3072 next 71\n",
      "2024-12-07 01:47:43.351351: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c1c800 of size 3072 next 73\n",
      "2024-12-07 01:47:43.351355: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c1d400 of size 12288 next 76\n",
      "2024-12-07 01:47:43.351359: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c20400 of size 3072 next 68\n",
      "2024-12-07 01:47:43.351362: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c21000 of size 3072 next 77\n",
      "2024-12-07 01:47:43.351366: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c21c00 of size 3072 next 78\n",
      "2024-12-07 01:47:43.351370: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c22800 of size 3072 next 65\n",
      "2024-12-07 01:47:43.351374: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c23400 of size 3072 next 82\n",
      "2024-12-07 01:47:43.351378: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c24000 of size 3072 next 83\n",
      "2024-12-07 01:47:43.351382: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c24c00 of size 3072 next 84\n",
      "2024-12-07 01:47:43.351386: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c25800 of size 3072 next 86\n",
      "2024-12-07 01:47:43.351390: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c26400 of size 3072 next 88\n",
      "2024-12-07 01:47:43.351394: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c27000 of size 12288 next 91\n",
      "2024-12-07 01:47:43.351398: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c2a000 of size 3072 next 90\n",
      "2024-12-07 01:47:43.351401: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c2ac00 of size 3072 next 92\n",
      "2024-12-07 01:47:43.351405: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c2b800 of size 3072 next 93\n",
      "2024-12-07 01:47:43.351409: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c2c400 of size 3072 next 66\n",
      "2024-12-07 01:47:43.351413: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c2d000 of size 3072 next 101\n",
      "2024-12-07 01:47:43.351417: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c2dc00 of size 3072 next 103\n",
      "2024-12-07 01:47:43.351421: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c2e800 of size 3072 next 105\n",
      "2024-12-07 01:47:43.351425: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c2f400 of size 3072 next 102\n",
      "2024-12-07 01:47:43.351428: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c30000 of size 3072 next 104\n",
      "2024-12-07 01:47:43.351432: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c30c00 of size 12288 next 107\n",
      "2024-12-07 01:47:43.351436: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c33c00 of size 3072 next 106\n",
      "2024-12-07 01:47:43.351440: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c34800 of size 3072 next 108\n",
      "2024-12-07 01:47:43.351444: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c35400 of size 3072 next 109\n",
      "2024-12-07 01:47:43.351448: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c36000 of size 3072 next 96\n",
      "2024-12-07 01:47:43.351453: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c36c00 of size 3072 next 114\n",
      "2024-12-07 01:47:43.351458: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c37800 of size 3072 next 115\n",
      "2024-12-07 01:47:43.351462: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c38400 of size 3072 next 116\n",
      "2024-12-07 01:47:43.351466: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c39000 of size 3072 next 118\n",
      "2024-12-07 01:47:43.351471: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c39c00 of size 3072 next 120\n",
      "2024-12-07 01:47:43.351476: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c3a800 of size 12288 next 123\n",
      "2024-12-07 01:47:43.351480: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c3d800 of size 3072 next 122\n",
      "2024-12-07 01:47:43.351485: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c3e400 of size 3072 next 124\n",
      "2024-12-07 01:47:43.351497: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c3f000 of size 3072 next 125\n",
      "2024-12-07 01:47:43.351505: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c3fc00 of size 3072 next 128\n",
      "2024-12-07 01:47:43.351515: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c40800 of size 3072 next 130\n",
      "2024-12-07 01:47:43.351520: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c41400 of size 3072 next 131\n",
      "2024-12-07 01:47:43.351524: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c42000 of size 3072 next 132\n",
      "2024-12-07 01:47:43.351528: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c42c00 of size 3072 next 134\n",
      "2024-12-07 01:47:43.351532: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c43800 of size 3072 next 136\n",
      "2024-12-07 01:47:43.351536: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c44400 of size 12288 next 139\n",
      "2024-12-07 01:47:43.351540: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c47400 of size 3072 next 138\n",
      "2024-12-07 01:47:43.351544: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c48000 of size 3072 next 140\n",
      "2024-12-07 01:47:43.351548: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c48c00 of size 3072 next 141\n",
      "2024-12-07 01:47:43.351551: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c49800 of size 3072 next 144\n",
      "2024-12-07 01:47:43.351555: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c4a400 of size 3072 next 146\n",
      "2024-12-07 01:47:43.351559: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c4b000 of size 3072 next 147\n",
      "2024-12-07 01:47:43.351563: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c4bc00 of size 3072 next 148\n",
      "2024-12-07 01:47:43.351567: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c4c800 of size 3072 next 150\n",
      "2024-12-07 01:47:43.351573: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c4d400 of size 3072 next 152\n",
      "2024-12-07 01:47:43.351577: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c4e000 of size 12288 next 155\n",
      "2024-12-07 01:47:43.351582: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c51000 of size 3072 next 158\n",
      "2024-12-07 01:47:43.351587: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c51c00 of size 3072 next 156\n",
      "2024-12-07 01:47:43.351592: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c52800 of size 3072 next 157\n",
      "2024-12-07 01:47:43.351597: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c53400 of size 3072 next 159\n",
      "2024-12-07 01:47:43.351602: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c54000 of size 3072 next 161\n",
      "2024-12-07 01:47:43.351608: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c54c00 of size 3072 next 162\n",
      "2024-12-07 01:47:43.351613: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c55800 of size 3072 next 163\n",
      "2024-12-07 01:47:43.351618: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c56400 of size 3072 next 165\n",
      "2024-12-07 01:47:43.351622: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c57000 of size 3072 next 167\n",
      "2024-12-07 01:47:43.351626: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c57c00 of size 12288 next 170\n",
      "2024-12-07 01:47:43.351630: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c5ac00 of size 3072 next 154\n",
      "2024-12-07 01:47:43.351634: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c5b800 of size 3072 next 171\n",
      "2024-12-07 01:47:43.351638: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c5c400 of size 3072 next 172\n",
      "2024-12-07 01:47:43.351642: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c5d000 of size 3072 next 160\n",
      "2024-12-07 01:47:43.351646: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c5dc00 of size 3072 next 180\n",
      "2024-12-07 01:47:43.351651: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c5e800 of size 3072 next 182\n",
      "2024-12-07 01:47:43.351663: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c5f400 of size 3072 next 184\n",
      "2024-12-07 01:47:43.351669: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c60000 of size 3072 next 181\n",
      "2024-12-07 01:47:43.351680: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c60c00 of size 3072 next 183\n",
      "2024-12-07 01:47:43.351689: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c61800 of size 12288 next 186\n",
      "2024-12-07 01:47:43.351694: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c64800 of size 3072 next 178\n",
      "2024-12-07 01:47:43.351699: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c65400 of size 3072 next 187\n",
      "2024-12-07 01:47:43.351704: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c66000 of size 3072 next 188\n",
      "2024-12-07 01:47:43.351709: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c66c00 of size 3072 next 176\n",
      "2024-12-07 01:47:43.351713: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c67800 of size 3072 next 196\n",
      "2024-12-07 01:47:43.351718: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c68400 of size 3072 next 198\n",
      "2024-12-07 01:47:43.351723: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c69000 of size 3072 next 200\n",
      "2024-12-07 01:47:43.351728: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c69c00 of size 3072 next 197\n",
      "2024-12-07 01:47:43.351733: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c6a800 of size 3072 next 199\n",
      "2024-12-07 01:47:43.351738: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c6b400 of size 12288 next 202\n",
      "2024-12-07 01:47:43.351743: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c6e400 of size 3072 next 194\n",
      "2024-12-07 01:47:43.351747: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c6f000 of size 3072 next 203\n",
      "2024-12-07 01:47:43.351752: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c6fc00 of size 3072 next 204\n",
      "2024-12-07 01:47:43.351762: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c70800 of size 3072 next 192\n",
      "2024-12-07 01:47:43.351768: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c71400 of size 256 next 220\n",
      "2024-12-07 01:47:43.351773: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c71500 of size 256 next 221\n",
      "2024-12-07 01:47:43.351778: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c71600 of size 256 next 228\n",
      "2024-12-07 01:47:43.351783: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c71700 of size 256 next 175\n",
      "2024-12-07 01:47:43.351788: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c71800 of size 256 next 223\n",
      "2024-12-07 01:47:43.351793: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c71900 of size 256 next 229\n",
      "2024-12-07 01:47:43.351798: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c71a00 of size 256 next 230\n",
      "2024-12-07 01:47:43.351803: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 501c71b00 of size 10496 next 210\n",
      "2024-12-07 01:47:43.351808: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c74400 of size 6144 next 214\n",
      "2024-12-07 01:47:43.351813: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c75c00 of size 12288 next 30\n",
      "2024-12-07 01:47:43.351819: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c78c00 of size 49152 next 216\n",
      "2024-12-07 01:47:43.351825: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c84c00 of size 81920 next 222\n",
      "2024-12-07 01:47:43.351836: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501c98c00 of size 98304 next 224\n",
      "2024-12-07 01:47:43.351842: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 501cb0c00 of size 98304 next 238\n",
      "2024-12-07 01:47:43.351847: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501cc8c00 of size 49152 next 237\n",
      "2024-12-07 01:47:43.351852: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 501cd4c00 of size 140544 next 225\n",
      "2024-12-07 01:47:43.351857: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501cf7100 of size 98304 next 227\n",
      "2024-12-07 01:47:43.351862: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 501d0f100 of size 1731328 next 14\n",
      "2024-12-07 01:47:43.351867: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 501eb5c00 of size 2663424 next 59\n",
      "2024-12-07 01:47:43.351872: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 502140000 of size 2359296 next 20\n",
      "2024-12-07 01:47:43.351877: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 502380000 of size 2359296 next 21\n",
      "2024-12-07 01:47:43.351882: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 5025c0000 of size 2359296 next 23\n",
      "2024-12-07 01:47:43.351887: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 502800000 of size 2359296 next 24\n",
      "2024-12-07 01:47:43.351892: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 502a40000 of size 2359296 next 28\n",
      "2024-12-07 01:47:43.351897: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 502c80000 of size 2359296 next 37\n",
      "2024-12-07 01:47:43.351902: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 502ec0000 of size 2359296 next 39\n",
      "2024-12-07 01:47:43.351907: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 503100000 of size 2359296 next 41\n",
      "2024-12-07 01:47:43.351912: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 503340000 of size 2359296 next 43\n",
      "2024-12-07 01:47:43.351916: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 503580000 of size 2359296 next 54\n",
      "2024-12-07 01:47:43.351922: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 5037c0000 of size 2359296 next 56\n",
      "2024-12-07 01:47:43.351927: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 503a00000 of size 2359296 next 58\n",
      "2024-12-07 01:47:43.351932: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 503c40000 of size 3072 next 5\n",
      "2024-12-07 01:47:43.351937: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 503c40c00 of size 11793408 next 31\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 1561/1562 [2:33:34<00:05,  5.90s/it]4780000 of size 2359296 next 179\n",
      "2024-12-07 01:47:43.351947: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 5049c0000 of size 2359296 next 191\n",
      "2024-12-07 01:47:43.351953: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 504c00000 of size 2359296 next 193\n",
      "2024-12-07 01:47:43.351957: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 504e40000 of size 2359296 next 75\n",
      "2024-12-07 01:47:43.351962: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 505080000 of size 9437184 next 46\n",
      "2024-12-07 01:47:43.351967: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 505980000 of size 9437184 next 47\n",
      "2024-12-07 01:47:43.351972: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 506280000 of size 9437184 next 49\n",
      "2024-12-07 01:47:43.351977: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 506b80000 of size 2359296 next 85\n",
      "2024-12-07 01:47:43.351982: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 506dc0000 of size 2359296 next 87\n",
      "2024-12-07 01:47:43.351988: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 507000000 of size 2359296 next 89\n",
      "2024-12-07 01:47:43.351992: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 507240000 of size 2359296 next 63\n",
      "2024-12-07 01:47:43.351998: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 507480000 of size 9437184 next 64\n",
      "2024-12-07 01:47:43.352002: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 507d80000 of size 2359296 next 69\n",
      "2024-12-07 01:47:43.352006: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 507fc0000 of size 2359296 next 80\n",
      "2024-12-07 01:47:43.352011: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 508200000 of size 2359296 next 98\n",
      "2024-12-07 01:47:43.352015: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 5084\n",
      "40000 of size 2359296 next 99\n",
      "2024-12-07 01:47:43.352019: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 508680000 of size 2359296 next 100\n",
      "2024-12-07 01:47:43.352024: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 5088c0000 of size 2359296 next 112\n",
      "2024-12-07 01:47:43.352028: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 508b00000 of size 2359296 next 177\n",
      "2024-12-07 01:47:43.352032: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 508d40000 of size 2359296 next 79\n",
      "2024-12-07 01:47:43.352038: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 508f80000 of size 13107200 next 18446744073709551615\n",
      "2024-12-07 01:47:43.352047: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1089] Next region of size 134217728\n",
      "2024-12-07 01:47:43.352052: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 509e00000 of size 9437184 next 81\n",
      "2024-12-07 01:47:43.352060: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50a700000 of size 2359296 next 117\n",
      "2024-12-07 01:47:43.352068: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50a940000 of size 2359296 next 119\n",
      "2024-12-07 01:47:43.352072: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50ab80000 of size 2359296 next 121\n",
      "2024-12-07 01:47:43.352077: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50adc0000 of size 2359296 next 94\n",
      "2024-12-07 01:47:43.352081: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50b000000 of size 9437184 next 95\n",
      "2024-12-07 01:47:43.352086: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50b900000 of size 9437184 next 97\n",
      "2024-12-07 01:47:43.352091: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50c200000 of size 2359296 next 133\n",
      "2024-12-07 01:47:43.352099: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50c440000 of size 2359296 next 135\n",
      "2024-12-07 01:47:43.352106: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50c680000 of size 2359296 next 137\n",
      "2024-12-07 01:47:43.352110: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50c8c0000 of size 2359296 next 110\n",
      "2024-12-07 01:47:43.352115: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50cb00000 of size 9437184 next 111\n",
      "2024-12-07 01:47:43.352142: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50d400000 of size 9437184 next 113\n",
      "2024-12-07 01:47:43.352150: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50dd00000 of size 2359296 next 149\n",
      "2024-12-07 01:47:43.352155: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50df40000 of size 2359296 next 151\n",
      "2024-12-07 01:47:43.352159: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50e180000 of size 2359296 next 153\n",
      "2024-12-07 01:47:43.352164: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50e3c0000 of size 2359296 next 126\n",
      "2024-12-07 01:47:43.352168: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50e600000 of size 9437184 next 127\n",
      "2024-12-07 01:47:43.352173: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50ef00000 of size 9437184 next 129\n",
      "2024-12-07 01:47:43.352178: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50f800000 of size 2359296 next 164\n",
      "2024-12-07 01:47:43.352182: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50fa40000 of size 2359296 next 166\n",
      "2024-12-07 01:47:43.352186: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50fc80000 of size 2359296 next 168\n",
      "2024-12-07 01:47:43.352190: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 50fec0000 of size 2359296 next 142\n",
      "2024-12-07 01:47:43.352195: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 510100000 of size 9437184 next 143\n",
      "2024-12-07 01:47:43.352199: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 510a00000 of size 9437184 next 145\n",
      "2024-12-07 01:47:43.352204: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 511300000 of size 11534336 next 18446744073709551615\n",
      "2024-12-07 01:47:43.352209: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1089] Next region of size 268435456\n",
      "2024-12-07 01:47:43.352213: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 511e00000 of size 37748736 next 240\n",
      "2024-12-07 01:47:43.352220: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 514200000 of size 37748736 next 239\n",
      "2024-12-07 01:47:43.352230: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 516600000 of size 46577664 next 173\n",
      "2024-12-07 01:47:43.352235: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 51926b800 of size 9437184 next 174\n",
      "2024-12-07 01:47:43.352239: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 519b6b800 of size 9437184 next 201\n",
      "2024-12-07 01:47:43.352244: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 51a46b800 of size 9437184 next 189\n",
      "2024-12-07 01:47:43.352249: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 51ad6b800 of size 9437184 next 190\n",
      "2024-12-07 01:47:43.352253: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 51b66b800 of size 2359296 next 195\n",
      "2024-12-07 01:47:43.352257: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 51b8ab800 of size 2359296 next 207\n",
      "2024-12-07 01:47:43.352266: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 51baeb800 of size 7077888 next 212\n",
      "2024-12-07 01:47:43.352273: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 51c1ab800 of size 2359296 next 27\n",
      "2024-12-07 01:47:43.352278: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 51c3eb800 of size 2359296 next 67\n",
      "2024-12-07 01:47:43.352282: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 51c62b800 of size 2359296 next 205\n",
      "2024-12-07 01:47:43.352286: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 51c86b800 of size 9437184 next 206\n",
      "2024-12-07 01:47:43.352291: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 51d16b800 of size 9437184 next 208\n",
      "2024-12-07 01:47:43.352295: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 51da6b800 of size 9437184 next 13\n",
      "2024-12-07 01:47:43.352300: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 51e36b800 of size 9437184 next 10\n",
      "2024-12-07 01:47:43.352305: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 51ec6b800 of size 9437184 next 169\n",
      "2024-12-07 01:47:43.352309: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 51f56b800 of size 9437184 next 185\n",
      "2024-12-07 01:47:43.352314: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 51fe6b800 of size 33114112 next 18446744073709551615\n",
      "2024-12-07 01:47:43.352319: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1089] Next region of size 536870912\n",
      "2024-12-07 01:47:43.352324: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 521e00000 of size 93763584 next 213\n",
      "2024-12-07 01:47:43.352329: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 52776b800 of size 150994944 next 241\n",
      "2024-12-07 01:47:43.352333: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 53076b800 of size 150994944 next 219\n",
      "2024-12-07 01:47:43.352338: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 53976b800 of size 141117440 next 18446744073709551615\n",
      "2024-12-07 01:47:43.352343: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1089] Next region of size 407892224\n",
      "2024-12-07 01:47:43.352347: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 542800000 of size 37748736 next 233\n",
      "2024-12-07 01:47:43.352352: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 544c00000 of size 37748736 next 209\n",
      "2024-12-07 01:47:43.352357: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 547000000 of size 37748736 next 242\n",
      "2024-12-07 01:47:43.352361: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 549400000 of size 150994944 next 217\n",
      "2024-12-07 01:47:43.352366: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 552400000 of size 143651072 next 18446744073709551615\n",
      "2024-12-07 01:47:43.352371: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114]      Summary of in-use Chunks by size: \n",
      "2024-12-07 01:47:43.352379: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 12 Chunks of size 256 totalling 3.0KiB\n",
      "2024-12-07 01:47:43.352384: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2024-12-07 01:47:43.352390: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 111 Chunks of size 3072 totalling 333.0KiB\n",
      "2024-12-07 01:47:43.352395: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 6144 totalling 6.0KiB\n",
      "2024-12-07 01:47:43.352401: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 11 Chunks of size 12288 totalling 132.0KiB\n",
      "2024-12-07 01:47:43.352406: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 15360 totalling 15.0KiB\n",
      "2024-12-07 01:47:43.352411: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 2 Chunks of size 49152 totalling 96.0KiB\n",
      "2024-12-07 01:47:43.352416: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 81920 totalling 80.0KiB\n",
      "2024-12-07 01:47:43.352422: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 2 Chunks of size 98304 totalling 192.0KiB\n",
      "2024-12-07 01:47:43.352427: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 2082304 totalling 1.99MiB\n",
      "2024-12-07 01:47:43.352432: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 48 Chunks of size 2359296 totalling 108.00MiB\n",
      "2024-12-07 01:47:43.352437: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 2663424 totalling 2.54MiB\n",
      "2024-12-07 01:47:43.352442: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 21 Chunks of size 9437184 totalling 189.00MiB\n",
      "2024-12-07 01:47:43.352447: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 11534336 totalling 11.00MiB\n",
      "2024-12-07 01:47:43.352452: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 11793408 totalling 11.25MiB\n",
      "2024-12-07 01:47:43.352457: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 13107200 totalling 12.50MiB\n",
      "2024-12-07 01:47:43.352462: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 33114112 totalling 31.58MiB\n",
      "2024-12-07 01:47:43.352467: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 5 Chunks of size 37748736 totalling 180.00MiB\n",
      "2024-12-07 01:47:43.352472: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 93763584 totalling 89.42MiB\n",
      "2024-12-07 01:47:43.352477: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 3 Chunks of size 150994944 totalling 432.00MiB\n",
      "2024-12-07 01:47:43.352483: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1121] Sum Total of in-use chunks: 1.04GiB\n",
      "2024-12-07 01:47:43.352487: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1123] Total bytes in pool: 1483731200 memory_limit_: 1483731355 available bytes: 155 curr_region_allocation_bytes_: 2147483648\n",
      "2024-12-07 01:47:43.352495: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1128] Stats: \n",
      "Limit:                      1483731355\n",
      "InUse:                      1122092800\n",
      "MaxInUse:                   1286506240\n",
      "NumAllocs:                     1002981\n",
      "MaxAllocSize:                200146944\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-12-07 01:47:43.352506: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:508] ************************__*************************************_________*******************_________\n",
      "2024-12-07 01:47:43.352530: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at softmax_op_gpu.cu.cc:200 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[48,12,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2024-12-07 01:47:43.352550: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[48,12,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer 'self' (type TFBertSelfAttention).\n\n{{function_node __wrapped__Softmax_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[48,12,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Softmax] name: \n\nCall arguments received by layer 'self' (type TFBertSelfAttention):\n  • hidden_states=tf.Tensor(shape=(48, 256, 768), dtype=float32)\n  • attention_mask=tf.Tensor(shape=(48, 1, 1, 256), dtype=float32)\n  • head_mask=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_value=None\n  • output_attentions=False\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m batch \u001b[38;5;241m=\u001b[39m X[i \u001b[38;5;241m*\u001b[39m BATCH_SIZE: (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m BATCH_SIZE] \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m ALL_BATCHES \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m X[i \u001b[38;5;241m*\u001b[39m BATCH_SIZE:]\n\u001b[1;32m      7\u001b[0m tokenized_inputs \u001b[38;5;241m=\u001b[39m preprocess_texts(batch\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m cls_embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# Признаки токена [CLS]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m dset\u001b[38;5;241m.\u001b[39mresize((dset\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m cls_embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-3/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-3/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:437\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-3/lib/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:1209\u001b[0m, in \u001b[0;36mTFBertModel.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;129m@unpack_inputs\u001b[39m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(BERT_INPUTS_DOCSTRING\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size, sequence_length\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;129m@add_code_sample_docstrings\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     training: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1188\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TFBaseModelOutputWithPoolingAndCrossAttentions, Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;124;03m    encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;124;03m        `past_key_values`). Set to `False` during training, `True` during generation\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1209\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-3/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:437\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-3/lib/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:969\u001b[0m, in \u001b[0;36mTFBertMainLayer.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    967\u001b[0m     head_mask \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers\n\u001b[0;32m--> 969\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    984\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(hidden_states\u001b[38;5;241m=\u001b[39msequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-3/lib/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:609\u001b[0m, in \u001b[0;36mTFBertEncoder.call\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    605\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[1;32m    607\u001b[0m past_key_value \u001b[38;5;241m=\u001b[39m past_key_values[i] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 609\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-3/lib/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:502\u001b[0m, in \u001b[0;36mTFBertLayer.call\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    491\u001b[0m     hidden_states: tf\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    499\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-3/lib/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:386\u001b[0m, in \u001b[0;36mTFBertAttention.call\u001b[0;34m(self, input_tensor, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    377\u001b[0m     input_tensor: tf\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    384\u001b[0m     training: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    385\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 386\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_output(\n\u001b[1;32m    397\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mself_outputs[\u001b[38;5;241m0\u001b[39m], input_tensor\u001b[38;5;241m=\u001b[39minput_tensor, training\u001b[38;5;241m=\u001b[39mtraining\n\u001b[1;32m    398\u001b[0m     )\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;66;03m# add attentions (possibly with past_key_value) if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-3/lib/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:299\u001b[0m, in \u001b[0;36mTFBertSelfAttention.call\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[1;32m    296\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39madd(attention_scores, attention_mask)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mstable_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(inputs\u001b[38;5;241m=\u001b[39mattention_probs, training\u001b[38;5;241m=\u001b[39mtraining)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-3/lib/python3.10/site-packages/transformers/tf_utils.py:72\u001b[0m, in \u001b[0;36mstable_softmax\u001b[0;34m(logits, axis, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03mStable wrapper that returns the same output as `tf.nn.softmax`, but that works reliably with XLA on CPU. It is\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03mmeant as a workaround for the [following issue](https://github.com/tensorflow/tensorflow/issues/55682), and will be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m        A Tensor. Has the same type and shape as logits.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# TODO: When the issue linked above gets sorted, add a check on TF version here and use the original function if\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# it has the fix. After we drop the support for unfixed versions, remove this function.\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer 'self' (type TFBertSelfAttention).\n\n{{function_node __wrapped__Softmax_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[48,12,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Softmax] name: \n\nCall arguments received by layer 'self' (type TFBertSelfAttention):\n  • hidden_states=tf.Tensor(shape=(48, 256, 768), dtype=float32)\n  • attention_mask=tf.Tensor(shape=(48, 1, 1, 256), dtype=float32)\n  • head_mask=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_value=None\n  • output_attentions=False\n  • training=False"
     ]
    }
   ],
   "source": [
    "with h5py.File(ECCOM_DATA_FEATURES_FILE, \"w\") as h5file:\n",
    "    dset = h5file.create_dataset(ECCOM_DATA_FEATURES_DATASET, shape=(0, 768), maxshape=(None, 768))\n",
    "    \n",
    "    for i in tqdm(range(0, ALL_BATCHES)):\n",
    "        batch = X[i * BATCH_SIZE: (i + 1) * BATCH_SIZE] if i < ALL_BATCHES - 1 else X[i * BATCH_SIZE:]\n",
    "        \n",
    "        tokenized_inputs = preprocess_texts(batch.tolist())\n",
    "        \n",
    "        outputs = backbone(\n",
    "            input_ids=tokenized_inputs['input_ids'],\n",
    "            attention_mask=tokenized_inputs['attention_mask']\n",
    "        )\n",
    "        \n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]  # Признаки токена [CLS]\n",
    "        \n",
    "        dset.resize((dset.shape[0] + cls_embeddings.shape[0]), axis=0)\n",
    "        dset[-cls_embeddings.shape[0]:] = cls_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28edbcec-b696-47de-89c3-381e0601d58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:  (49952, 768)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(ECCOM_DATA_FEATURES_FILE, \"r\") as h5file:\n",
    "    dataset = h5file[ECCOM_DATA_FEATURES_DATASET]\n",
    "    partial_data = dataset[:, :]\n",
    "    print(\"Example: \", partial_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b4b64-ee2e-4a2a-8d81-161f8bf1aae4",
   "metadata": {},
   "source": [
    "Run the backbone on the sequences and save the extracted features. The extracted features should be a vector containing information about the whole text. If the features are too large to be stored in memory, lazily save them on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2543fcff-46b0-4c05-b297-c3102f998205",
   "metadata": {},
   "source": [
    "# Prepare train/test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c985c7d-9123-477e-9988-15ff5a7b26eb",
   "metadata": {},
   "source": [
    "Split your data (extracted features and labels) into train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e08dc56-16d3-492e-a537-96a66e2b3d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_TOKENS = 49952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e614fd5e-f470-4973-b1cf-1abdce7e8016",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_embedings = None\n",
    "\n",
    "with h5py.File(ECCOM_DATA_FEATURES_FILE, \"r\") as h5file:\n",
    "    dataset = h5file[ECCOM_DATA_FEATURES_DATASET]\n",
    "    description_embedings = dataset[:LEN_TOKENS, :768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c49581b5-cdcc-4380-8a0b-d1258e49fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = y[:LEN_TOKENS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2a7748a-f977-4648-963b-93cea1f0eb7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49952,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5460c05a-f5e7-4935-a6ff-074ca21fa01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(description_embedings, labels, test_size=0.3, random_state=RANDOM_SEED)\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.5, random_state=RANDOM_SEED) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a65639-50d0-4f89-8bfe-33b7a483e406",
   "metadata": {},
   "source": [
    "Prepare `tf.data.Dataset` or some other way for the data to be used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07849c2e-c7d1-4620-9590-962fa554ed8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE).shuffle(buffer_size=BATCH_SIZE)\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((X_validation, y_validation)).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f74abd-ce1a-4a68-be6c-89b1abd0caa5",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78885e31-c91e-4ee5-acbf-00a107069da9",
   "metadata": {},
   "source": [
    "Build a simple model. The model should accept an extracted feature vector and return a vector of class logits (or probabilities). Model should only have a couple (or even 1) layer with weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4522f1a0-1555-4dcd-9117-3e975d6f781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    input_layer = tf.keras.layers.Input(shape=(768,), batch_size=BATCH_SIZE)\n",
    "    output = tf.keras.layers.Dense(CLASS_COUNT, activation=\"softmax\")(input_layer)\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2145fff1-2446-4b2e-a08b-f9002bf645ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cacc4e-9876-40ff-8cd0-f1a4bfbc5c1a",
   "metadata": {},
   "source": [
    "Compile the model. Choose loss and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad74527b-c793-40da-af8e-66172db170f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e08006d-be0a-4828-aca9-a9f37218b8de",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e100a4fe-8737-4806-b2a5-e4fa93cb745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY_DIR = './history'\n",
    "os.makedirs(HISTORY_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4a5109d-e562-486a-9ada-4e3e3b658757",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = os.path.join(HISTORY_DIR, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2e68d3a-07eb-419f-b311-dd20273d88a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name: str):\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(logdir, f'model-{model_name}.keras'),\n",
    "        save_best_only=True\n",
    "    )\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        os.path.join(logdir, f'logs-{model_name}.keras'),    \n",
    "    )\n",
    "\n",
    "    return model_checkpoint_callback, tensorboard_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2994d4a-7243-4a42-bdd1-6bfb19a4c285",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback, tensorboard_callback = get_callbacks('bert-backbone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a30787b-7ac4-4dd3-9ea5-62c94e9aa628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733527650.341645  150702 service.cc:148] XLA service 0x7efab8248770 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1733527650.342750  150702 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce GT 1030, Compute Capability 6.1\n",
      "2024-12-07 09:27:30.373401: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1733527650.426676  150702 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  12/1093 [..............................] - ETA: 5s - loss: 1.7670 - accuracy: 0.2318   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733527650.548433  150702 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1093/1093 [==============================] - 5s 4ms/step - loss: 0.9948 - accuracy: 0.6270 - val_loss: 0.7282 - val_accuracy: 0.7798\n",
      "Epoch 2/20\n",
      "1093/1093 [==============================] - 4s 4ms/step - loss: 0.6045 - accuracy: 0.8356 - val_loss: 0.5356 - val_accuracy: 0.8545\n",
      "Epoch 3/20\n",
      "1093/1093 [==============================] - 4s 4ms/step - loss: 0.4725 - accuracy: 0.8734 - val_loss: 0.4501 - val_accuracy: 0.8768\n",
      "Epoch 4/20\n",
      "1093/1093 [==============================] - 4s 4ms/step - loss: 0.4080 - accuracy: 0.8881 - val_loss: 0.4039 - val_accuracy: 0.8863\n",
      "Epoch 5/20\n",
      "1093/1093 [==============================] - 4s 4ms/step - loss: 0.3710 - accuracy: 0.8956 - val_loss: 0.3753 - val_accuracy: 0.8907\n",
      "Epoch 6/20\n",
      "1093/1093 [==============================] - 5s 5ms/step - loss: 0.3474 - accuracy: 0.9009 - val_loss: 0.3562 - val_accuracy: 0.8948\n",
      "Epoch 7/20\n",
      "1093/1093 [==============================] - 4s 4ms/step - loss: 0.3311 - accuracy: 0.9042 - val_loss: 0.3424 - val_accuracy: 0.8983\n",
      "Epoch 8/20\n",
      "1093/1093 [==============================] - 4s 4ms/step - loss: 0.3190 - accuracy: 0.9072 - val_loss: 0.3319 - val_accuracy: 0.9020\n",
      "Epoch 9/20\n",
      "1093/1093 [==============================] - 5s 4ms/step - loss: 0.3098 - accuracy: 0.9100 - val_loss: 0.3235 - val_accuracy: 0.9046\n",
      "Epoch 10/20\n",
      "1093/1093 [==============================] - 5s 4ms/step - loss: 0.3024 - accuracy: 0.9121 - val_loss: 0.3168 - val_accuracy: 0.9055\n",
      "Epoch 11/20\n",
      "1093/1093 [==============================] - 5s 4ms/step - loss: 0.2963 - accuracy: 0.9131 - val_loss: 0.3112 - val_accuracy: 0.9070\n",
      "Epoch 12/20\n",
      "1093/1093 [==============================] - 4s 4ms/step - loss: 0.2911 - accuracy: 0.9150 - val_loss: 0.3063 - val_accuracy: 0.9083\n",
      "Epoch 13/20\n",
      "1093/1093 [==============================] - 5s 5ms/step - loss: 0.2866 - accuracy: 0.9162 - val_loss: 0.3020 - val_accuracy: 0.9095\n",
      "Epoch 14/20\n",
      "1093/1093 [==============================] - 5s 4ms/step - loss: 0.2828 - accuracy: 0.9171 - val_loss: 0.2985 - val_accuracy: 0.9100\n",
      "Epoch 15/20\n",
      "1093/1093 [==============================] - 4s 4ms/step - loss: 0.2793 - accuracy: 0.9183 - val_loss: 0.2950 - val_accuracy: 0.9118\n",
      "Epoch 16/20\n",
      "1093/1093 [==============================] - 4s 4ms/step - loss: 0.2762 - accuracy: 0.9194 - val_loss: 0.2921 - val_accuracy: 0.9129\n",
      "Epoch 17/20\n",
      "1093/1093 [==============================] - 4s 4ms/step - loss: 0.2734 - accuracy: 0.9202 - val_loss: 0.2894 - val_accuracy: 0.9127\n",
      "Epoch 18/20\n",
      "1093/1093 [==============================] - 5s 4ms/step - loss: 0.2708 - accuracy: 0.9214 - val_loss: 0.2871 - val_accuracy: 0.9139\n",
      "Epoch 19/20\n",
      "1093/1093 [==============================] - 5s 4ms/step - loss: 0.2684 - accuracy: 0.9220 - val_loss: 0.2847 - val_accuracy: 0.9139\n",
      "Epoch 20/20\n",
      "1093/1093 [==============================] - 6s 5ms/step - loss: 0.2663 - accuracy: 0.9224 - val_loss: 0.2827 - val_accuracy: 0.9151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7efa9042a1d0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, validation_data=validation_dataset, epochs=EPOCH_COUNT, callbacks=[model_checkpoint_callback, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3980f062-220a-4a11-b9f5-3b0e41804533",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6008a2-9882-44f2-946c-23827aa5fa49",
   "metadata": {},
   "source": [
    "Evalute the model on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b4ddf9e-5710-4c7f-ac58-4d94742c0d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.2646 - accuracy: 0.9250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.26464518904685974, 0.9249966740608215]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96f4ae8-6b6c-4c4d-94dc-be8062b7e982",
   "metadata": {},
   "source": [
    "Plot confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b092eae2-5f9a-42cc-ba04-ae9b9f67df7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5b38889-e4ca-491f-9992-c5810e6aff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3e24681-e424-4e8e-a76d-80e23b3d9ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[0, 1, 2, 3])\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6997d559-de7f-4e67-96f1-85df047dcbb7",
   "metadata": {},
   "source": [
    "Write a function to classify a piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "21a748d8-b5a2-4f90-a3d8-ee36d69d34fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text: str) -> tuple[int | str, np.ndarray]:\n",
    "    tokenized_input = preprocess_texts(text)\n",
    "\n",
    "    outputs = backbone(\n",
    "        input_ids=tokenized_input['input_ids'],\n",
    "        attention_mask=tokenized_input['attention_mask']\n",
    "    )\n",
    "        \n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "    predictions = model.predict(cls_embeddings)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return LABELS[int(predicted_labels)], predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2043ad-8f1b-4c17-8f11-23487f66db5f",
   "metadata": {},
   "source": [
    "Evaluate the model on text not present in training and test data (come up with the text yourself). Try to get an input for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b27c5386-cc1b-4040-a5b7-cb8d26965fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_150544/3462582729.py:14: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return LABELS[int(predicted_labels)], predictions\n"
     ]
    }
   ],
   "source": [
    "result_class = classify_text(ads['home_care'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f85b6d8-d287-4ae2-96b9-ad26621d5cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Household',\n",
       " array([[0.48929223, 0.23004185, 0.20513321, 0.07553267]], dtype=float32))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49c775-eded-4a49-b7f4-9903185a7b9d",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726c617b-7110-4761-9806-975d5f84bf41",
   "metadata": {},
   "source": [
    "Write a function that computes word impact on text label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e55243d7-df29-4ff6-8db5-ff029269cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_INDEX = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7aed5cd7-c770-4341-a2fe-211b55b6100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions, labels = get_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "01a46658-8770-4d53-94ec-b7651dbde221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_impact(text: str) -> list[tuple[str, np.ndarray]]:\n",
    "    def get_predicted_probabilities(text):\n",
    "        tokenized_input = preprocess_texts(text)\n",
    "    \n",
    "        outputs = backbone(\n",
    "            input_ids=tokenized_input['input_ids'],\n",
    "            attention_mask=tokenized_input['attention_mask']\n",
    "        )\n",
    "            \n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "        predictions = model.predict(cls_embeddings)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    all_sequence_predicted_probabilities = get_predicted_probabilities(text)[0]\n",
    "\n",
    "    impacts = []\n",
    "    words = text.split()\n",
    "\n",
    "    print(words)\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        modified_text = \" \".join(words[:i] + words[i+1:])\n",
    "        \n",
    "        predicted_probabilities = get_predicted_probabilities(modified_text)[0]\n",
    "        difference = all_sequence_predicted_probabilities - predicted_probabilities\n",
    "        impacts.append((word, difference))\n",
    "\n",
    "    return impacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a55960df-7527-41de-ae04-fa3c3102296b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n",
      "['SAF', \"'Floral'\", 'Framed', 'Painting', '(Wood,', '30', 'inch', 'x', '10', 'inch,', 'Special', 'Effect', 'UV', 'Print', 'Textured,', 'SAO297)', 'Painting', 'made', 'up', 'in', 'synthetic', 'frame', 'with', 'UV', 'textured', 'print', 'which', 'gives', 'multi', 'effects', 'and', 'attracts', 'towards', 'it.', 'This', 'is', 'an', 'special', 'series', 'of', 'paintings', 'which', 'makes', 'your', 'wall', 'very', 'beautiful', 'and', 'gives', 'a', 'royal', 'touch', '(A', 'perfect', 'gift', 'for', 'your', 'special', 'ones).']\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "impacts = get_words_impact(descriptions[ITEM_INDEX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7e0f117d-7a33-4840-990e-0659ba8e6c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "true_label_seq = labels[ITEM_INDEX]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6dbd69-ea3c-41df-943c-16a379384b16",
   "metadata": {},
   "source": [
    "Try to find out words that make text have a specific label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "84e72ccb-63cf-43ed-bc2f-5026897e721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_biggest_impact(label, impacts):\n",
    "    words = []\n",
    "    predicted_probabilities = []\n",
    "\n",
    "    for impact in impacts:\n",
    "        words.append(impact[0])\n",
    "        predicted_probabilities.append(impact[1])\n",
    "\n",
    "    words = np.array(words)\n",
    "    predicted_probabilities = np.array(predicted_probabilities)\n",
    "\n",
    "    true_label_probabilities = predicted_probabilities[:,label]\n",
    "    max_impact_index = np.argwhere(true_label_probabilities > 0).reshape(-1)\n",
    "\n",
    "    return words[max_impact_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3b7629c2-d39d-4ea7-b726-31230faee0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Textured,', 'made', 'special', 'which', 'wall', 'and', 'royal',\n",
       "       'perfect', 'special', 'ones).'], dtype='<U9')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_biggest_impact(true_label_seq, impacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1d3b6930-dd3f-4865-b8e3-203415885202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SAF 'Floral' Framed Painting (Wood, 30 inch x 10 inch, Special Effect UV Print Textured, SAO297) Painting made up in synthetic frame with UV textured print which gives multi effects and attracts towards it. This is an special series of paintings which makes your wall very beautiful and gives a royal touch (A perfect gift for your special ones).\""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions[ITEM_INDEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d5be15c-bf79-4019-bbf2-23cc4f058299",
   "metadata": {},
   "outputs": [],
   "source": [
    "ads = {\n",
    "    \"home_care\": [\n",
    "        \"Transform your home into a sanctuary with our eco-friendly cleaning solutions!\",\n",
    "        \"Simplify your daily chores with our innovative home care products.\",\n",
    "        \"Say goodbye to stubborn stains—welcome to effortless cleaning!\",\n",
    "        \"Fresh scents, spotless results. Try our all-natural home care line today!\",\n",
    "        \"Keep your home cozy and clean with our premium fabric care solutions.\",\n",
    "        \"Cleaning made easy! Discover tools that save you time and effort.\",\n",
    "        \"Your perfect cleaning companion awaits—shop now and save!\",\n",
    "        \"Make every corner of your home shine with our expert care products.\",\n",
    "        \"Gentle on surfaces, tough on dirt. Choose smart cleaning!\",\n",
    "        \"Breathe easier—experience cleaner air with our advanced purifiers.\"\n",
    "    ],\n",
    "    \"electronics\": [\n",
    "        \"Upgrade your tech game with the latest gadgets at unbeatable prices!\",\n",
    "        \"Experience cinema-quality sound and visuals at home—shop our electronics now!\",\n",
    "        \"Stay connected with cutting-edge devices designed for your lifestyle.\",\n",
    "        \"The future is here: discover next-gen smart home electronics today!\",\n",
    "        \"Elevate your productivity with our state-of-the-art laptops and accessories.\",\n",
    "        \"Capture life’s best moments in HD—find the perfect camera for you.\",\n",
    "        \"Stream, surf, and play with lightning-fast devices. Shop now!\",\n",
    "        \"Power meets elegance in our premium smartphone collection.\",\n",
    "        \"Stay ahead with innovative wearables that fit your lifestyle.\",\n",
    "        \"Save big on top-brand electronics—limited time only!\"\n",
    "    ],\n",
    "    \"books\": [\n",
    "        \"Embark on your next adventure—browse our best-selling novels now!\",\n",
    "        \"Expand your horizons with inspiring non-fiction books.\",\n",
    "        \"Cozy up with the perfect story—find your new favorite read today!\",\n",
    "        \"From thrillers to romance, we’ve got every genre covered.\",\n",
    "        \"Feed your mind and soul—shop our curated book collections.\",\n",
    "        \"Unlock new worlds—explore the latest fantasy and sci-fi titles!\",\n",
    "        \"Knowledge is power—find the perfect book to learn and grow.\",\n",
    "        \"Love paperbacks? Check out our exclusive deals on top reads!\",\n",
    "        \"Discover timeless classics and modern masterpieces in one place.\",\n",
    "        \"Books that move you—shop stories that stay with you forever.\"\n",
    "    ],\n",
    "    \"clothing\": [\n",
    "        \"Step out in style with our trendy and affordable clothing!\",\n",
    "        \"Comfort meets fashion—shop our best-selling casual wear.\",\n",
    "        \"Elevate your wardrobe with timeless classics and bold designs.\",\n",
    "        \"Find your perfect fit—explore our new arrivals in all sizes!\",\n",
    "        \"Get cozy this season with our soft and stylish knitwear.\",\n",
    "        \"Make a statement—shop bold and unique fashion pieces now!\",\n",
    "        \"From work to weekend, we’ve got the outfits you’ll love.\",\n",
    "        \"Dress your best without breaking the bank—shop now!\",\n",
    "        \"Refresh your look with our exclusive deals on seasonal must-haves.\",\n",
    "        \"Confidence starts with the right outfit—find yours today!\"\n",
    "    ]\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
